{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG)\n",
        "\n",
        "The field of NLP has witnessed significant advancements in recent years, and RAG is one such exciting development. In this series of notebooks, we will cover the fundamentals of RAG, its architecture, and practical implementations. We will also work on some hands-on examples to grasp the concepts better.\n",
        "\n",
        "Let us embark on this journey together and explore the powerful capabilities of RAG!\n",
        "\n",
        "Happy learning!\n"
      ],
      "metadata": {
        "id": "8EsqvDIUnTb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Jupyter notebook, we will be working with the llama_index and langchain libraries to perform document indexing and retrieval using GPT-3.5-turbo, an advanced language model. The purpose of this notebook is to demonstrate how to set up the environment, load documents, and create an index for efficient document retrieval.\n",
        "\n",
        "Before we proceed, please note that we will be using the OpenAI API to leverage the capabilities of the GPT-3.5-turbo model. As a security measure, remember never to reveal your API keys directly in code. Instead, use environment variables or other secure means to store sensitive information.\n",
        "\n",
        "We will follow these steps:\n",
        "\n",
        "- Import necessary classes and functions from the llama_index and langchain libraries.\n",
        "- Set up the OpenAI API key using an environment variable and directly for demonstration purposes (please avoid doing this in production code).\n",
        "- Load data from the specified directory, where we assume the documents are stored. You may adjust the path according to your data location.\n",
        "- Initialize the LLMPredictor with the desired GPT-3.5-turbo model and temperature setting.\n",
        "- Create a ServiceContext using the initialized predictor.\n",
        "- Index the loaded documents using the created service context to enable efficient document retrieval.\n",
        "- Now that you have an overview of the tasks we'll be performing, let's proceed with the document loading and indexing process. Happy coding! üöÄ"
      ],
      "metadata": {
        "id": "tPV_GdwznTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install llama_index which is a popular middleware used in many GenAI applications\n",
        "!pip install llama_index"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T11:27:30.211143Z",
          "iopub.execute_input": "2023-07-31T11:27:30.211976Z",
          "iopub.status.idle": "2023-07-31T11:27:50.705056Z",
          "shell.execute_reply.started": "2023-07-31T11:27:30.211937Z",
          "shell.execute_reply": "2023-07-31T11:27:50.703681Z"
        },
        "trusted": true,
        "id": "RrVnxuchnTcB",
        "outputId": "34c0403e-23d7-4e3d-a8aa-ef2790985894"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting llama_index\n  Downloading llama_index-0.7.16-py3-none-any.whl (626 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tiktoken (from llama_index)\n  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama_index) (0.5.9)\nCollecting langchain>=0.0.218 (from llama_index)\n  Downloading langchain-0.0.247-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sqlalchemy>=2.0.15 in /opt/conda/lib/python3.10/site-packages (from llama_index) (2.0.17)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llama_index) (1.23.5)\nRequirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama_index) (8.2.2)\nCollecting openai>=0.26.4 (from llama_index)\n  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama_index) (1.5.3)\nRequirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from llama_index) (1.26.15)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama_index) (2023.6.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama_index) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama_index) (4.6.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from llama_index) (4.12.2)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from llama_index) (1.5.6)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (6.0)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (4.0.2)\nCollecting langsmith<0.1.0,>=0.0.11 (from langchain>=0.0.218->llama_index)\n  Downloading langsmith-0.0.15-py3-none-any.whl (30 kB)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (2.8.4)\nCollecting openapi-schema-pydantic<2.0,>=1.2 (from langchain>=0.0.218->llama_index)\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (1.10.9)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.218->llama_index) (2.31.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama_index) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama_index) (1.5.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai>=0.26.4->llama_index) (4.65.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=2.0.15->llama_index) (2.0.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->llama_index) (2.3.2.post1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama_index) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama_index) (2023.3)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->llama_index) (2023.6.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama_index) (1.3.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (21.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.218->llama_index) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.218->llama_index) (2023.5.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (3.0.9)\nInstalling collected packages: tiktoken, openapi-schema-pydantic, langsmith, openai, langchain, llama_index\nSuccessfully installed langchain-0.0.247 langsmith-0.0.15 llama_index-0.7.16 openai-0.27.8 openapi-schema-pydantic-1.2.4 tiktoken-0.4.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes and functions from the llama_index and langchain libraries\n",
        "from llama_index import (\n",
        "    GPTVectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        "    LLMPredictor,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Import the openai library and os module to set the API key\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# SECURITY ALERT: Never reveal your API keys directly in code. Use environment variables or other secure means.\n",
        "# Here, we're setting the OpenAI API key both using an environment variable and directly (demonstration purposes only)\n",
        "os.environ['OPENAI_API_KEY'] = 'YOU-API-KEY'\n",
        "openai.api_key = 'YOU-API-KEY'\n",
        "\n",
        "# Notify the user that the document loading process has begun\n",
        "print(\"started the loading document process...\")\n",
        "\n",
        "# Read the data from the specified directory. Change './boiler_docs/' to your desired path.\n",
        "documents = SimpleDirectoryReader('/kaggle/input/nlp-and-llm-related-arxiv-papers/').load_data()\n",
        "\n",
        "# Initialize the LLMPredictor with the desired GPT-3.5-turbo model and temperature setting\n",
        "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
        "\n",
        "# Create a ServiceContext using the initialized predictor\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "\n",
        "# Notify the user that the indexing process has begun\n",
        "print(\"started the indexing process...\")\n",
        "\n",
        "# Create an index using the loaded documents and the created service context\n",
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T11:29:05.063370Z",
          "iopub.execute_input": "2023-07-31T11:29:05.063883Z",
          "iopub.status.idle": "2023-07-31T11:55:42.712879Z",
          "shell.execute_reply.started": "2023-07-31T11:29:05.063842Z",
          "shell.execute_reply": "2023-07-31T11:55:42.711779Z"
        },
        "trusted": true,
        "id": "VRNLr_HDnTcD",
        "outputId": "3d3b7a4a-ff5b-4a36-dde0-f6f02ece58e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "started the loading document process...\nstarted the indexing process...\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the created index to the disk at the specified location\n",
        "print(\"storing the index to disk\")\n",
        "index.storage_context.persist(persist_dir=\"/kaggle/working/nlp-and-llm-related-arxiv-papers-documents-index\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T11:56:45.528197Z",
          "iopub.execute_input": "2023-07-31T11:56:45.528704Z",
          "iopub.status.idle": "2023-07-31T11:59:42.320764Z",
          "shell.execute_reply.started": "2023-07-31T11:56:45.528665Z",
          "shell.execute_reply": "2023-07-31T11:59:42.319502Z"
        },
        "trusted": true,
        "id": "86uDSMwGnTcD",
        "outputId": "855a81a6-bfe4-4abb-f07d-a9b2e77fafff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "storing the index to disk\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of various prompting techniques. Please provide an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T11:59:42.325638Z",
          "iopub.execute_input": "2023-07-31T11:59:42.326035Z",
          "iopub.status.idle": "2023-07-31T12:00:04.865991Z",
          "shell.execute_reply.started": "2023-07-31T11:59:42.326001Z",
          "shell.execute_reply": "2023-07-31T12:00:04.864931Z"
        },
        "trusted": true,
        "id": "soDTQMg3nTcE",
        "outputId": "3720a281-8526-4780-d34a-f9b922996f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExecutive Summary:\nThis context provides a table summarizing various prompting techniques for designing prompts in large language models. The techniques include making the prompt detailed, specifying the expertise of the model, providing task descriptions, using contextual information, and utilizing demonstrations. The table also mentions the related principles for each technique.\n\nPrompts Techniques:\n- Make the prompt as detailed as possible, specifying the required length and including major storyline and conclusion while omitting unimportant details.\n- Specify the expertise of the model in the prompt, such as \"You are a sophisticated expert in the domain of computer science.\"\n- Instruct the model on what it should do rather than what it should not do.\n- Use a prompt format like \"Question: Short Answer:\" to avoid generating excessively long outputs.\n- Retrieve relevant documents via a search engine and concatenate them into the prompt as reference for factual knowledge questions.\n- Use special marks like quotation and line breaks to highlight important parts in the prompt.\n- Clearly describe the required intermediate steps for complex tasks.\n- Provide a detailed description of the scoring standard with examples as reference when asking the model to provide scores for a text.\n- Instruct the model with explanations about the generated result conditioned on context, especially for tasks like making recommendations based on purchase history.\n- Use well-formatted in-context exemplars to guide the model, especially for complex formats.\n- Use prompts like \"Let's think step-by-step\" for few-shot chain-of-thought prompting, separating examples with line breaks.\n- Retrieve similar examples in context to supply task-specific knowledge for the model.\n- Maintain diversity in the in-context exemplars within the prompt, either in terms of questions or solutions.\n- Decompose in-context exemplars into multi-turn messages for chat-based models to match the human-chatbot conversation format.\n- Complex and informative in-context exemplars can help the model answer complex questions.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of ActionCLIP.Please provide an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:02:11.257173Z",
          "iopub.execute_input": "2023-07-31T12:02:11.257697Z",
          "iopub.status.idle": "2023-07-31T12:02:39.540667Z",
          "shell.execute_reply.started": "2023-07-31T12:02:11.257659Z",
          "shell.execute_reply": "2023-07-31T12:02:39.539366Z"
        },
        "trusted": true,
        "id": "oWxgyo_3nTcE",
        "outputId": "de596bde-389c-4302-8887-a37871a00643"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExecutive Summary:\nActionCLIP is a multimodal learning framework for video action recognition that leverages both video and text information. It introduces a new paradigm called \"pre-train, prompt, and fine-tune\" to utilize pre-trained models and reduce pre-training costs. ActionCLIP achieves impressive results in both general and zero-shot/few-shot action recognition tasks, demonstrating the potential of the multimodal learning framework.\n\nDetailed Summary:\n- ActionCLIP proposes a new perspective for action recognition by considering it as a video-text multimodal learning problem.\n- Unlike traditional approaches that treat action recognition as a video classification problem, ActionCLIP utilizes semantic information from label texts to enhance performance.\n- The framework follows a \"pre-train, prompt, and fine-tune\" paradigm, allowing it to reuse pre-trained models trained on large-scale web data, thereby reducing pre-training costs.\n- ActionCLIP implementation achieves superior performance in general action recognition as well as zero-shot/few-shot action recognition tasks.\n- The framework is evaluated on datasets like Kinetics-400 and Charades, demonstrating its effectiveness in multi-label video classification.\n- The results show that ActionCLIP outperforms other models, achieving a top performance of 44.3 mAP in the Kinetics-400 dataset.\n- The authors believe that further improvements can be made by increasing the input frames, using larger models, and higher input resolutions.\n- The proposed multimodal learning framework and the new paradigm of ActionCLIP highlight the potential of language modeling in action recognition.\n- The work aims to provide a new perspective and raise attention to the importance of language modeling in action recognition tasks.\n\nBullet Points:\n- ActionCLIP is a multimodal learning framework for video action recognition.\n- It considers action recognition as a video-text multimodal learning problem.\n- ActionCLIP utilizes semantic information from label texts to enhance performance.\n- The framework follows a \"pre-train, prompt, and fine-tune\" paradigm to reduce pre-training costs.\n- ActionCLIP achieves superior performance in general and zero-shot/few-shot action recognition tasks.\n- It outperforms other models, achieving a top performance of 44.3 mAP in the Kinetics-400 dataset.\n- ActionCLIP highlights the potential of language modeling in action recognition.\n- The work aims to provide a new perspective and raise attention to language modeling in action recognition tasks.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of AI-assisted coding. Please provide an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:03:30.749526Z",
          "iopub.execute_input": "2023-07-31T12:03:30.750745Z",
          "iopub.status.idle": "2023-07-31T12:04:00.235667Z",
          "shell.execute_reply.started": "2023-07-31T12:03:30.750700Z",
          "shell.execute_reply": "2023-07-31T12:04:00.234037Z"
        },
        "trusted": true,
        "id": "CiiHU1EXnTcE",
        "outputId": "0e5d09ee-5c1e-454f-8834-4c67f5b31c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExecutive Summary:\nAI-assisted coding refers to the use of artificial intelligence technologies to assist in the process of writing code. It involves the use of machine learning algorithms and natural language processing techniques to automate certain coding tasks and provide suggestions and recommendations to developers. AI-assisted coding aims to improve productivity, accuracy, and efficiency in software development by reducing manual effort and enhancing code quality.\n\nDetailed Summary:\nAI-assisted coding is a technique that leverages artificial intelligence technologies to aid developers in writing code. It involves the use of machine learning algorithms and natural language processing techniques to analyze code and provide intelligent suggestions and recommendations. The goal of AI-assisted coding is to enhance productivity and efficiency in software development by automating repetitive tasks and improving code quality.\n\nSome key points about AI-assisted coding include:\n\n1. Automation: AI-assisted coding automates various coding tasks, such as code completion, error detection, and refactoring. It can automatically generate code snippets based on context and provide suggestions for completing code blocks.\n\n2. Intelligent Suggestions: AI algorithms analyze code patterns and learn from existing code repositories to provide intelligent suggestions and recommendations. These suggestions can include code optimizations, alternative implementations, and best practices.\n\n3. Code Quality Improvement: AI-assisted coding tools can detect potential bugs, security vulnerabilities, and performance issues in code. They can also enforce coding standards and style guidelines to ensure consistent and maintainable code.\n\n4. Language Support: AI-assisted coding tools support multiple programming languages and frameworks. They can adapt to the specific syntax and conventions of different languages and provide language-specific suggestions and recommendations.\n\n5. Integration with IDEs: AI-assisted coding tools are often integrated with popular integrated development environments (IDEs) to provide a seamless coding experience. They can be used as plugins or extensions within the IDE, allowing developers to access AI-powered features directly within their coding environment.\n\n6. Continuous Learning: AI-assisted coding systems continuously learn from user feedback and improve their suggestions over time. They can adapt to individual coding styles and preferences, providing personalized recommendations to developers.\n\nIn summary, AI-assisted coding is a powerful approach that combines artificial intelligence and coding to enhance productivity and code quality in software development. It automates repetitive tasks, provides intelligent suggestions, and improves code accuracy and efficiency. By leveraging AI technologies, developers can streamline their coding process and focus on higher-level tasks, ultimately leading to faster and more reliable software development.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of how ChatGPT is a Jack of all trades but master of none. Please provide an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:04:36.398886Z",
          "iopub.execute_input": "2023-07-31T12:04:36.399332Z",
          "iopub.status.idle": "2023-07-31T12:05:16.198052Z",
          "shell.execute_reply.started": "2023-07-31T12:04:36.399300Z",
          "shell.execute_reply": "2023-07-31T12:05:16.196700Z"
        },
        "trusted": true,
        "id": "s68Pnv7CnTcF",
        "outputId": "1b181521-11f0-4f80-e3d9-877130160957"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExecutive Summary:\nChatGPT is a versatile language model that can perform various tasks but lacks expertise in any specific domain. While it can handle different tasks, its accuracy and quality may vary, leading to inconsistent results. ChatGPT is interactive and creative, providing multiple answers, but it can also be repetitive. It is currently in the beta testing stage and has hidden biases and limitations in its reasoning and understanding abilities.\n\nDetailed Summary:\n- ChatGPT is a general-purpose language model that can perform a wide range of tasks, making it a \"Jack of all trades.\"\n- However, its performance in these tasks may not be accurate or consistent, leading to a drop in quality compared to more specialized models.\n- ChatGPT is interactive and can engage in conversations, but its answers may not always be accurate or reliable.\n- It has the ability to generate creative responses and provide multiple answers, but it can also be repetitive, giving the same answer repeatedly.\n- ChatGPT is currently in the beta testing stage, which means it is still being refined and improved.\n- It does not engage in passive conversation and requires specific prompts or questions to generate responses.\n- In terms of stability, ChatGPT is considered stable and is already in production.\n- However, it lacks explainability, as it only provides raw answers without explaining the reasoning behind them.\n- Recent methods have shown that ChatGPT can exhibit biases and discrimination in its responses, highlighting the need for further improvement.\n- The performance of ChatGPT heavily relies on the quality of task-specific prompts, and prompt tuning is not available for this model.\n- The prompts used for ChatGPT are manually selected based on stability and accuracy, but this approach may not always guarantee optimal performance.\n- ChatGPT's ability to understand various data formats is evaluated by designing prompts that include all the necessary information for the task.\n- The tasks considered in this paper are relatively structured and have simple expected results, reflecting typical machine learning solutions.\n- The focus is on the correctness of ChatGPT's analysis and inference, rather than the quality of the generated text as perceived by the user.\n- ChatGPT's performance compared to the best recent models (SOTA) in solving NLP analytical tasks may vary depending on the task type.\n- The difficulty of the tasks also affects ChatGPT's ability to solve them accurately.\n- Personalization through a few-shot approach can potentially improve ChatGPT's reasoning and inference quality.\n- The impact of context on processing multiple questions or prompts is also considered.\n- The availability and exploitation of data for training ChatGPT can impact its performance.\n- Post-processing activities may be necessary to improve the quality of ChatGPT's output for analytical tasks.\n- The internal policy of ChatGPT providers and biases within the model can result in inadequate responses to certain prompts.\n\nBullet Points:\n- ChatGPT is a versatile language model but lacks expertise in specific domains.\n- Its performance in different tasks may vary, leading to inconsistent results.\n- ChatGPT is interactive and creative, providing multiple answers, but it can also be repetitive.\n- It is currently in the beta testing stage and has hidden biases and limitations in its reasoning and understanding abilities.\n- Prompt engineering and tuning are not available for ChatGPT, affecting its performance.\n- The model's ability to understand various data formats is evaluated through prompt design.\n- The focus is on the correctness of analysis and inference, rather than the quality of generated text.\n- ChatGPT's performance compared to SOTA models varies depending on the task type and difficulty.\n- Personalization and context can impact ChatGPT's reasoning and inference quality.\n- The availability and exploitation of data for training affect ChatGPT's performance.\n- Post-processing activities may be necessary to improve the quality of ChatGPT's output.\n- The model's internal policy and biases can result in inadequate responses to certain prompts.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of fine-tuning of language models. Discuss techniques of fine-tuning, why is fine-tuning important and its applications. Please provide an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:05:54.285975Z",
          "iopub.execute_input": "2023-07-31T12:05:54.286415Z",
          "iopub.status.idle": "2023-07-31T12:06:15.094794Z",
          "shell.execute_reply.started": "2023-07-31T12:05:54.286384Z",
          "shell.execute_reply": "2023-07-31T12:06:15.093599Z"
        },
        "trusted": true,
        "id": "RYUbMb8bnTcF",
        "outputId": "eca747dc-7a82-4c71-af31-20db5a1206bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExecutive Summary:\nFine-tuning of language models is a crucial process that involves adapting pre-trained models to specific tasks or domains. This process enhances the models' performance and enables them to generalize better. Several techniques have been developed for fine-tuning, including Supervised Instruction Tuning, Continual Learning, Parameter-Efficient Fine-Tuning, and Semi-Supervised Fine-Tuning. These techniques address different challenges and aim to make the fine-tuning process more efficient and effective. Fine-tuning has various applications, such as instruction following, generating responses, summarization, translation, and more.\n\nKey Points:\n- Fine-tuning of language models involves adapting pre-trained models to specific tasks or domains.\n- Techniques for fine-tuning include Supervised Instruction Tuning, Continual Learning, Parameter-Efficient Fine-Tuning, and Semi-Supervised Fine-Tuning.\n- Supervised Instruction Tuning involves fine-tuning with task instruction supervision.\n- Continual Learning aims to incorporate new data into models without catastrophic forgetting.\n- Parameter-Efficient Fine-Tuning focuses on efficient adaptation of language models.\n- Semi-Supervised Fine-Tuning addresses the problem of insufficient supervision from interaction messages.\n- Fine-tuning improves the performance and task generalization abilities of language models.\n- Fine-tuning has applications in instruction following, response generation, summarization, translation, and more.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Write a detailed summary of fine-tuning of language models. Discuss techniques of fine-tuning, why is fine-tuning important and its applications. Write a detailed blog post that is sufficiently technical and is targeted to an audience that has machine learning and NLP knowledge? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:07:00.014975Z",
          "iopub.execute_input": "2023-07-31T12:07:00.015399Z",
          "iopub.status.idle": "2023-07-31T12:07:34.493840Z",
          "shell.execute_reply.started": "2023-07-31T12:07:00.015366Z",
          "shell.execute_reply": "2023-07-31T12:07:34.492293Z"
        },
        "trusted": true,
        "id": "I6TugBtLnTcF",
        "outputId": "859c7d5b-df43-41db-b282-26041d341ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nFine-tuning is a crucial process in natural language processing (NLP) that involves adapting pre-trained language models to specific tasks or domains. It plays a significant role in improving model performance and enhancing task generalization abilities. In this blog post, we will explore the concept of fine-tuning in detail, discussing various techniques, its importance, and applications.\n\nFine-tuning a language model involves updating its parameters using task-specific or domain-specific data. This process allows the model to learn from new data without forgetting previously learned information. There are several commonly employed methods for fine-tuning language models:\n\n1. Supervised Instruction Tuning: This technique focuses on adapting language models for instruction following and enhancing their task generalization abilities. It involves fine-tuning the model using data that provides task instruction supervision. This approach has been widely studied and has shown promising results in various applications.\n\n2. Continual Learning: Continual learning aims to infuse new data into language models without catastrophic forgetting. It ensures that the model retains its previously learned knowledge while incorporating new information. This technique is particularly useful when dealing with evolving datasets or when the model needs to adapt to changing contexts.\n\n3. Parameter-Efficient Fine-Tuning: This approach focuses on efficiently adapting language models by minimizing the computational resources required. It aims to strike a balance between model performance and resource utilization. By optimizing the fine-tuning process, this technique enables faster adaptation of language models to new tasks or domains.\n\n4. Semi-Supervised Fine-Tuning: In some cases, the interaction message may not provide adequate supervision to train the model. Semi-supervised fine-tuning addresses this issue by leveraging unlabeled data. It combines labeled and unlabeled data to improve model performance and tackle the problem of insufficient supervision.\n\nFine-tuning is important because it allows language models to be customized for specific tasks or domains, leading to improved performance and better task generalization. By adapting the model to the target task, it can better understand and generate relevant outputs. Fine-tuning also enables the model to learn from new data, making it more adaptable to evolving contexts and datasets.\n\nThe applications of fine-tuning in NLP are vast. It can be used in various domains, such as healthcare, finance, customer service, and more. For example, in healthcare, fine-tuning can be applied to language models to extract information from medical records, assist in diagnosis, or generate patient reports. In finance, fine-tuning can help analyze financial documents, predict market trends, or automate trading strategies. The possibilities are endless, and fine-tuning allows language models to be tailored to specific industry requirements.\n\nIn conclusion, fine-tuning is a crucial process in NLP that enables language models to be adapted to specific tasks or domains. It involves updating the model's parameters using task-specific or domain-specific data. By fine-tuning, models can improve their performance, enhance task generalization, and adapt to evolving contexts. The techniques discussed, such as supervised instruction tuning, continual learning, parameter-efficient fine-tuning, and semi-supervised fine-tuning, offer different approaches to achieve effective adaptation. The applications of fine-tuning in various domains highlight its significance in real-world NLP applications.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"what are examples of multi-modal embeddings? Discuss the importance of multi-modal embeddings and its applications. Write a detailed blog post that is sufficiently technical and is targeted to an audience that has machine learning and NLP knowledge? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:13:26.126360Z",
          "iopub.execute_input": "2023-07-31T12:13:26.126904Z",
          "iopub.status.idle": "2023-07-31T12:14:06.870782Z",
          "shell.execute_reply.started": "2023-07-31T12:13:26.126862Z",
          "shell.execute_reply": "2023-07-31T12:14:06.869410Z"
        },
        "trusted": true,
        "id": "oQZY0FplnTcG",
        "outputId": "f2f44b25-c47b-4111-fdc6-5a04ee2061e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nTitle: Exploring the Power of Multi-Modal Embeddings in Machine Learning and NLP\n\nIntroduction:\nIn the era of advanced machine learning techniques, the combination of multiple modalities has emerged as a powerful approach to enhance the performance of various tasks. Multi-modal embeddings, which efficiently and effectively combine different modalities such as audio, video, and text, have become a hot topic in the field of Natural Language Processing (NLP). In this blog post, we will delve into the importance of multi-modal embeddings and explore their applications in machine learning and NLP.\n\nImportance of Multi-Modal Embeddings:\nMulti-modal embeddings play a crucial role in bridging the gap between different modalities and enabling machines to understand and process information from various senses, including vision, hearing, and language. By combining different types of data simultaneously, multi-modal models can leverage the complementary nature of modalities, leading to improved performance and richer representations. Here are a few reasons why multi-modal embeddings are important:\n\n1. Enhanced Performance: Different modalities often provide unique and complementary information. By combining these modalities, multi-modal embeddings can capture a more comprehensive understanding of the data, leading to enhanced performance in various tasks such as image classification, speech recognition, and sentiment analysis.\n\n2. Richer Representations: Multi-modal embeddings enable machines to capture both the visual and textual aspects of data, resulting in richer representations. For example, in visual question answering tasks, combining NLP and computer vision allows models to understand both textual and visual information, leading to more accurate and context-aware answers.\n\n3. Robustness to Noisy Data: Multi-modal embeddings can help mitigate the impact of noisy or incomplete data by leveraging information from multiple modalities. By considering multiple sources of information, models can make more informed decisions and handle uncertainties more effectively.\n\nApplications of Multi-Modal Embeddings:\nThe applications of multi-modal embeddings are vast and diverse, spanning across various domains. Here are a few notable examples:\n\n1. Grounding with Multiple Modalities: Grounding refers to the process of connecting language to the real world. By combining multiple modalities such as audio, video, and text, models can better understand and interpret language in context. This has applications in areas like driver alertness measurement, depression detection, and deceptive behavior detection.\n\n2. Medical Image Translation: Multi-modal embeddings have shown promising results in medical image-to-image translation tasks. By combining magnetic resonance imaging (MRI) data with other modalities, models can generate accurate translations and improve the quality of medical images. This has implications in areas like oncology and radiotherapy.\n\n3. Visual Question Answering: Multi-modal embeddings are extensively used in visual question answering tasks, where models combine textual and visual information to answer questions about images or videos. This has applications in areas like image captioning, object recognition, and scene understanding.\n\nConclusion:\nMulti-modal embeddings have revolutionized the field of machine learning and NLP by enabling the combination of different modalities to enhance performance and capture richer representations. By efficiently integrating audio, video, text, and other modalities, models can achieve better results in various tasks and domains. The applications of multi-modal embeddings are diverse and impactful, ranging from medical imaging to visual question answering. As researchers continue to explore and develop new techniques, the potential of multi-modal embeddings in advancing machine learning and NLP is boundless.\n\nIn this blog post, we have only scratched the surface of the vast field of multi-modal embeddings. As technology progresses, we can expect to witness even more exciting developments and applications in the future. So, stay tuned and keep exploring the power of multi-modal embeddings in machine learning and NLP!\n\nReferences:\n- A PhD Students Perspective on Research in NLP in the Era of Very Large Language Models.pdf\n- A COMPREHENSIVE SURVEY ON APPLICATIONS OF TRANSFORMERS FOR DEEP LEARNING TASKS.pdf\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Summarize the sparks of AGI paper and identify what are some tasks that GPT-4 excels in at human level? Write a detailed blog post that is sufficiently technical and is targeted to an audience that has machine learning and NLP knowledge? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:15:05.931790Z",
          "iopub.execute_input": "2023-07-31T12:15:05.932242Z",
          "iopub.status.idle": "2023-07-31T12:15:43.830731Z",
          "shell.execute_reply.started": "2023-07-31T12:15:05.932210Z",
          "shell.execute_reply": "2023-07-31T12:15:43.829448Z"
        },
        "trusted": true,
        "id": "5tZlThQhnTcG",
        "outputId": "6b966aa6-6c61-4ec0-8a6f-ec6ada58ef86"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nTitle: Sparks of Artificial General Intelligence: Unveiling the Potential of GPT-4\n\nIntroduction:\nArtificial General Intelligence (AGI) has long been a goal in the field of machine learning and natural language processing (NLP). In a recent paper titled \"Sparks of Artificial General Intelligence,\" researchers shed light on the capabilities and limitations of GPT-4, a cutting-edge language model. This blog post aims to summarize the key findings of the paper and highlight the tasks in which GPT-4 excels at a human-level performance.\n\nSummary of the Paper:\nThe paper presents an in-depth analysis of GPT-4, comparing its performance with ChatGPT, a previous state-of-the-art language model. The researchers found that GPT-4 surpasses ChatGPT in terms of generating impressive outputs. Moreover, GPT-4's performance is at least comparable, if not superior, to that of a human in certain tasks.\n\nTasks in which GPT-4 Excels at Human-Level Performance:\n1. Language Translation: GPT-4 demonstrates remarkable proficiency in translating text from one language to another. Its ability to capture the nuances and context of different languages enables it to produce high-quality translations.\n\n2. Sentiment Analysis: GPT-4 exhibits exceptional skills in understanding and analyzing the sentiment expressed in a given text. It can accurately identify positive, negative, or neutral sentiments, making it a valuable tool for sentiment analysis tasks.\n\n3. Text Summarization: GPT-4's proficiency in generating concise and coherent summaries of lengthy texts is noteworthy. It can effectively extract the most important information and present it in a coherent manner, saving time and effort for users.\n\n4. Question Answering: GPT-4 showcases impressive capabilities in answering questions based on given contexts. It can comprehend complex queries and provide accurate and relevant answers, rivaling human performance in this task.\n\n5. Creative Writing: GPT-4 exhibits a flair for creative writing, generating engaging and imaginative content. It can produce captivating stories, poems, and articles, demonstrating its ability to mimic human creativity.\n\nLimitations and Challenges:\nWhile GPT-4's performance is commendable, the paper also highlights certain limitations and challenges associated with the model:\n\n1. Conceptual Leaps: GPT-4 struggles with tasks that require conceptual leaps, which are often associated with human genius. It lacks the ability to make intuitive connections or think beyond the information it has been trained on.\n\n2. Transparency and Consistency: The model occasionally hallucinates, fabricates facts, and produces inconsistent content. It lacks a mechanism to verify the consistency of its generated output with the training data or within its own reasoning process. This poses challenges in establishing trust and collaboration with users.\n\nConclusion:\nThe sparks of AGI paper provides valuable insights into the capabilities and limitations of GPT-4, a state-of-the-art language model. GPT-4 demonstrates remarkable performance in various tasks, including language translation, sentiment analysis, text summarization, question answering, and creative writing. However, it falls short in tasks that require conceptual leaps and faces challenges related to transparency and consistency. As researchers continue to push the boundaries of AGI, it is crucial to address these limitations and strive for models that exhibit both human-level performance and a deeper understanding of complex tasks.\n\nBy leveraging the power of GPT-4, we can unlock new possibilities in machine learning and NLP, revolutionizing industries such as translation, sentiment analysis, and content generation. However, it is essential to remain cognizant of the model's limitations and work towards addressing them to ensure reliable and trustworthy AI systems.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"How can Large language models be used in recommendation engines? Write a detailed blog post that is sufficiently technical and is targeted to an audience that has machine learning and NLP knowledge? Provide a concrete example in ecommerce where a Large Language Model can be used for recommendations? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:16:59.913969Z",
          "iopub.execute_input": "2023-07-31T12:16:59.914568Z",
          "iopub.status.idle": "2023-07-31T12:17:44.355816Z",
          "shell.execute_reply.started": "2023-07-31T12:16:59.914523Z",
          "shell.execute_reply": "2023-07-31T12:17:44.354462Z"
        },
        "trusted": true,
        "id": "buZ7i4zDnTcG",
        "outputId": "4911dc4a-7e7d-4fd0-9928-2c1a6e3633d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nTitle: Enhancing Recommendation Engines with Large Language Models\n\nIntroduction:\nLarge language models (LLMs) have gained significant attention in recent years due to their remarkable capabilities in natural language processing (NLP) tasks. These models, such as GPT-3 and GPT-4, have the potential to revolutionize recommendation engines by providing more accurate and personalized recommendations to users. In this blog post, we will explore how LLMs can be utilized in recommendation engines, specifically in the context of e-commerce. We will delve into the technical aspects of integrating LLMs into recommendation systems and provide a concrete example to illustrate their effectiveness.\n\nUnderstanding Recommendation Engines:\nRecommendation engines are algorithms that analyze user data and provide personalized suggestions or recommendations. These systems are widely used in e-commerce platforms to enhance user experience, increase customer engagement, and drive sales. Traditional recommendation engines rely on collaborative filtering, content-based filtering, or hybrid approaches to generate recommendations. However, these methods often face challenges in capturing the nuances of user preferences and providing accurate suggestions.\n\nUtilizing Large Language Models in Recommendation Engines:\nLLMs can significantly enhance recommendation engines by leveraging their language understanding capabilities. These models excel at processing and comprehending textual data, enabling them to capture the semantic meaning and context of user preferences. By integrating LLMs into recommendation systems, we can overcome the limitations of traditional approaches and provide more accurate and personalized recommendations.\n\n1. Natural Language Understanding:\nLLMs can analyze user queries, product descriptions, and reviews to gain a deep understanding of the textual data. This allows the model to capture the subtle nuances of user preferences and generate recommendations that align with their specific needs. For example, if a user searches for \"comfortable running shoes for long-distance,\" the LLM can interpret the query and recommend suitable products based on the user's requirements.\n\n2. Contextual Recommendations:\nLLMs can consider the broader context of user interactions and preferences. By analyzing previous search history, purchase behavior, and browsing patterns, the model can generate recommendations that align with the user's current context. For instance, if a user has recently purchased camping gear, the LLM can recommend related products such as hiking boots or outdoor clothing.\n\n3. Sentiment Analysis:\nLLMs can analyze user reviews and feedback to understand sentiment and preferences. By considering the sentiment expressed in reviews, the model can recommend products that align with the user's preferences. For example, if a user frequently leaves positive reviews for organic skincare products, the LLM can recommend similar products with high customer satisfaction ratings.\n\n4. Personalized Product Descriptions:\nLLMs can generate personalized product descriptions based on user preferences. By incorporating user-specific information, such as demographics, purchase history, and browsing behavior, the model can tailor the product descriptions to match the user's interests. This personalized approach enhances the user experience and increases the likelihood of conversion.\n\nExample: Enhancing E-commerce Recommendations with LLMs:\nLet's consider an example in the e-commerce domain, specifically a fashion retail platform. By integrating LLMs into the recommendation engine, the platform can provide personalized fashion recommendations to users. Here's how it works:\n\n1. User Query:\nA user searches for \"summer dresses for beach vacations.\"\n\n2. LLM Analysis:\nThe LLM analyzes the query and understands the user's intent. It considers factors such as the season, occasion, and user preferences.\n\n3. Contextual Recommendations:\nBased on the user's search history and browsing patterns, the LLM generates recommendations for summer dresses suitable for beach vacations. It considers factors like fabric, style, and customer reviews to provide accurate suggestions.\n\n4. Personalized Product Descriptions:\nThe LLM generates personalized product descriptions for each recommended dress. It incorporates user-specific information, such as body type, color preferences, and size, to tailor the descriptions to the user's needs.\n\n5. Sentiment Analysis:\nThe LLM analyzes customer reviews and feedback for each recommended dress. It considers the sentiment expressed in the reviews to ensure high customer satisfaction.\n\nConclusion:\nLarge language models have the potential to revolutionize recommendation engines in the e-commerce domain. By leveraging their natural language understanding capabilities, LLMs can provide more accurate and personalized recommendations to users. The integration of LLMs into recommendation systems enables contextual recommendations, sentiment analysis, and personalized product descriptions. This enhances the user experience, increases customer engagement, and drives sales. As LLM technology continues to advance, we can expect even more sophisticated recommendation engines that cater to individual preferences and deliver highly relevant suggestions.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Provide a concrete example in ecommerce where a Large Language Model can be used for recommendations using the three-step prompting approach? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:29:06.534696Z",
          "iopub.execute_input": "2023-07-31T12:29:06.535524Z",
          "iopub.status.idle": "2023-07-31T12:29:34.654732Z",
          "shell.execute_reply.started": "2023-07-31T12:29:06.535482Z",
          "shell.execute_reply": "2023-07-31T12:29:34.653257Z"
        },
        "trusted": true,
        "id": "-jQXvfe7nTcG",
        "outputId": "3f7f4476-90a1-414b-c115-7708225412cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nExample:\nPrompt: \"Please provide personalized product recommendations for a user based on their browsing history and preferences. The recommendations should be in the form of a list, with each product's name, description, and price. Additionally, include a short explanation for why each product is recommended. Use the following three-step prompting approach:\n\nStep 1 - Retrieve the user's browsing history and preferences:\n- Retrieve the user's browsing history from the database.\n- Retrieve the user's preferences, such as favorite categories or brands.\n\nStep 2 - Analyze the user's data and generate initial recommendations:\n- Analyze the user's browsing history to identify their interests and preferences.\n- Use machine learning algorithms to generate initial recommendations based on the user's data.\n\nStep 3 - Refine the recommendations and provide explanations:\n- Filter the initial recommendations based on the user's preferences.\n- For each recommended product, provide a short explanation of why it is recommended, highlighting its features or how it matches the user's preferences.\n\nRecommended Products:\n1. Product Name: XYZ Laptop\n   Description: This laptop is perfect for users who need high performance for gaming and video editing. It has a powerful processor, dedicated graphics card, and a large storage capacity. Price: $1500\n   Explanation: Based on the user's browsing history, it seems they are interested in gaming and multimedia tasks. The XYZ Laptop offers excellent performance for these activities, making it a suitable recommendation.\n\n2. Product Name: ABC Smart TV\n   Description: This smart TV offers a stunning 4K display, smart features, and a wide range of streaming options. Price: $800\n   Explanation: Considering the user's preference for entertainment, the ABC Smart TV provides an immersive viewing experience with its high-resolution display and smart features.\n\n3. Product Name: PQR Fitness Tracker\n   Description: This fitness tracker monitors your heart rate, tracks your workouts, and provides personalized health insights. Price: $100\n   Explanation: Based on the user's interest in fitness-related products, the PQR Fitness Tracker is a great recommendation. It offers advanced tracking features and helps users stay motivated in their fitness journey.\n\nPlease note that these recommendations are generated based on the user's browsing history and preferences. The prices mentioned are subject to change and may vary depending on the retailer.\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Provide solutions on how I can ChatGPT in languages other than English? Is there a way to use the english translation of my prompt which is in Hindi?  Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:31:13.155697Z",
          "iopub.execute_input": "2023-07-31T12:31:13.156216Z",
          "iopub.status.idle": "2023-07-31T12:31:32.238761Z",
          "shell.execute_reply.started": "2023-07-31T12:31:13.156176Z",
          "shell.execute_reply": "2023-07-31T12:31:32.237528Z"
        },
        "trusted": true,
        "id": "sS7ChiNGnTcG",
        "outputId": "2ab3baec-b6bd-4346-92a4-d76153a661eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nTo use ChatGPT in languages other than English, you can follow these solutions:\n\n1. Translate the prompt: If your prompt is in a language other than English, you can translate it to English before using it with ChatGPT. There are various online translation tools available that can help you with this. Once you have the translated prompt in English, you can use it with ChatGPT.\n\n2. Use multilingual models: OpenAI has released multilingual models that can understand and generate text in multiple languages. You can use these models to directly interact with ChatGPT in languages other than English. These models support languages such as Spanish, French, German, Italian, Dutch, Portuguese, Russian, Chinese, Japanese, Korean, and more.\n\n3. Incorporate language-specific prompts: If you want to use the English translation of your prompt, you can include language-specific keywords or phrases in your prompt to guide the model's response. For example, if your prompt is in Hindi and you want to use the English translation, you can add a line like \"Translate the following code from Hindi to English:\" before providing the code. This can help the model understand the context and generate a relevant response.\n\nRemember to format your prompt properly and provide clear instructions to the model. Line breaks can be used to improve the formatting and make the output more readable.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Provide a fairly comprehensive list of NLP tasks that LLMs can execute? Incluide details from the paper SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks and other papers that discuss NLP tasks. In your response provide a long list of tasks and their descriptions?  Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:37:32.071901Z",
          "iopub.execute_input": "2023-07-31T12:37:32.072317Z",
          "iopub.status.idle": "2023-07-31T12:38:02.027370Z",
          "shell.execute_reply.started": "2023-07-31T12:37:32.072285Z",
          "shell.execute_reply": "2023-07-31T12:38:02.025917Z"
        },
        "trusted": true,
        "id": "23ccnUAUnTcG",
        "outputId": "31fc504a-c5b2-4d70-973f-30cfa6e82c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nHere is a fairly comprehensive list of NLP tasks that LLMs (Language Models) can execute, based on the context information provided:\n\n1. Question Answering: Answering questions based on given context or knowledge.\n2. Classification: Assigning labels or categories to text data.\n3. Named Entity Recognition: Identifying and classifying named entities (e.g., person, organization, location) in text.\n4. Sentiment Analysis: Determining the sentiment or emotion expressed in text.\n5. Text Summarization: Generating a concise summary of a longer text.\n6. Machine Translation: Translating text from one language to another.\n7. Text Generation: Generating coherent and contextually relevant text.\n8. Text Completion: Predicting missing or next words in a given text.\n9. Text Classification: Assigning predefined categories or labels to text documents.\n10. Text Similarity: Measuring the similarity between two or more texts.\n11. Text Segmentation: Dividing a text into smaller segments or units.\n12. Text Clustering: Grouping similar texts together based on their content.\n13. Text Categorization: Assigning texts to predefined categories or topics.\n14. Text Sentiment Analysis: Analyzing the sentiment or opinion expressed in text.\n15. Text Emotion Detection: Detecting emotions conveyed in text.\n16. Text Intent Recognition: Identifying the intent or purpose behind a given text.\n17. Text Paraphrasing: Rewriting a given text while preserving its original meaning.\n18. Text Coherence Assessment: Evaluating the coherence or logical flow of a text.\n19. Text Style Transfer: Transforming the style or tone of a given text.\n20. Text Topic Modeling: Identifying the main topics or themes in a collection of texts.\n21. Text Dependency Parsing: Analyzing the syntactic structure and dependencies within a sentence.\n22. Text Coreference Resolution: Resolving references to the same entity in a text.\n23. Text Named Entity Linking: Linking named entities in text to their corresponding knowledge base entries.\n24. Text Relation Extraction: Identifying and extracting relationships between entities in text.\n25. Text Knowledge Graph Construction: Building a knowledge graph from textual data.\n26. Text Document Classification: Classifying entire documents into predefined categories.\n27. Text Document Summarization: Generating a summary of an entire document.\n28. Text Document Clustering: Grouping similar documents together based on their content.\n29. Text Document Recommendation: Recommending relevant documents based on user preferences.\n30. Text Document Sentiment Analysis: Analyzing the sentiment expressed in an entire document.\n\nThese tasks cover a wide range of natural language understanding and generation capabilities of LLMs.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"Provide a fairly comprehensive list of tasks that LLMs can perform that may not be entirely NLP related. Include examples from papers related StructGPT, Gorilla, Starcoder and others.  In your response provide a long list of tasks and their descriptions?  Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:39:31.542976Z",
          "iopub.execute_input": "2023-07-31T12:39:31.543449Z",
          "iopub.status.idle": "2023-07-31T12:40:37.776950Z",
          "shell.execute_reply.started": "2023-07-31T12:39:31.543410Z",
          "shell.execute_reply": "2023-07-31T12:40:37.775530Z"
        },
        "trusted": true,
        "id": "cM0u2HRRnTcG",
        "outputId": "702d3202-50f3-4f83-8bee-867848079f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nLLMs (Large Language Models) have the potential to perform a wide range of tasks beyond NLP (Natural Language Processing). Here is a comprehensive list of tasks that LLMs can perform:\n\n1. Chatbot: LLMs like ChatGPT can act as conversational agents, engaging in dialogue with users and providing informative and reliable responses.\n\n2. Data Annotation: LLMs can be used as annotators to label or annotate data for various tasks, such as sentiment analysis, named entity recognition, or part-of-speech tagging.\n\n3. Data Generation: LLMs can generate synthetic data for data augmentation, improving the performance of models in tasks like text classification or machine translation.\n\n4. Quality Assessment: LLMs can be used to evaluate the quality of generated text in NLG (Natural Language Generation) tasks like summarization and translation. They can provide human-like assessments and align well with human judgments.\n\n5. Visual Question Answering (VQA): Fine-tuned multimodal models, such as BEiT and PaLI, dominate tasks like VQA and image captioning, where the models need to handle multiple data types like text, images, audio, video, actions, and robotics.\n\n6. Instruction-Following Demonstrations: Texts generated by LLMs, like GPT-3.5, can be used as human-like instruction-following demonstrations to train other language models.\n\n7. Interpretability: LLMs possess the ability to reason and provide explanations for their predictions, which enhances their interpretability and helps understand their decision-making process.\n\n8. Regression Tasks: LLMs, although not their primary objective, can be used for regression tasks involving predicting continuous values. However, their performance in such tasks may be inferior to fine-tuned models like RoBERTa.\n\n9. Real-World \"Tasks\": LLMs are better suited to handle real-world scenarios compared to fine-tuned models. They can handle noisy and unstructured input, tasks not formalized by academia, and follow users' instructions effectively.\n\n10. Ambiguity Handling: LLMs are equipped to handle ambiguity, understand context, and process noisy input due to their training on diverse datasets encompassing various writing styles, languages, and domains.\n\n11. Task Adaptation: LLMs demonstrate a strong ability to generate open-domain responses, making them adaptable to new or unexpected user requests. Fine-tuned models, on the other hand, may struggle with adapting to such scenarios.\n\n12. Instruction Tuning: Techniques like instruction tuning and human alignment tuning further enhance LLMs' capabilities to comprehend and follow user instructions, generating helpful, coherent, and consistent responses.\n\nThese tasks and their descriptions highlight the versatility and potential of LLMs in various domains beyond NLP.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"\"\"\n",
        "Can you summarize the papers related to Code synthesis, code generation? Keywords like CoPilot, Starcoder, SQL-PALM and so on. I want a summary of what these LLMs can do and how can a student who wants to learn coding\n",
        "start to use them? Give me a step by step approach on how these LLMs can be used to teach students how to code?\n",
        "\"\"\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:42:16.266126Z",
          "iopub.execute_input": "2023-07-31T12:42:16.266635Z",
          "iopub.status.idle": "2023-07-31T12:42:47.250111Z",
          "shell.execute_reply.started": "2023-07-31T12:42:16.266599Z",
          "shell.execute_reply": "2023-07-31T12:42:47.249008Z"
        },
        "trusted": true,
        "id": "NrOHtE_tnTcG",
        "outputId": "2c8138e2-2bc2-4e44-f0c4-55b47774b978"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nThe context information provided includes two papers related to code synthesis and code generation using Large Language Models (LLMs). The first paper, titled \"SheetCopilot- Bringing Software Productivity to the Next Level through Large Language Models,\" focuses on spreadsheet manipulation using LLMs. The second paper, titled \"A Survey of Large Language Models,\" discusses various tasks that LLMs can perform, including code synthesis.\n\nLLMs have shown strong abilities in generating both natural language text and formal language, such as computer programs. Code synthesis refers to the generation of code that satisfies specific conditions. Existing LLMs have been evaluated based on the quality of the generated code by calculating the pass rate against test cases.\n\nTo teach students how to code using LLMs, a step-by-step approach can be followed:\n\n1. Familiarize students with the concept of LLMs: Introduce students to the concept of Large Language Models and explain how they can be used for code generation and synthesis.\n\n2. Provide access to LLM-based tools: Students can be given access to tools like CoPilot, Starcoder, SQL-PALM, or similar platforms that utilize LLMs for code generation. These tools can assist students in writing code by suggesting completions, providing examples, and offering guidance.\n\n3. Start with simple coding exercises: Begin with simple coding exercises that require students to write basic programs. Encourage students to use the LLM-based tools to generate code snippets or complete solutions for these exercises.\n\n4. Gradually increase complexity: As students gain confidence and proficiency, gradually increase the complexity of the coding exercises. Encourage them to rely on the LLM-based tools for generating more complex code structures and solutions.\n\n5. Emphasize understanding and customization: While using LLM-based tools, emphasize the importance of understanding the generated code and customizing it to meet specific requirements. Encourage students to analyze the generated code, modify it as needed, and experiment with different approaches.\n\n6. Practice and hands-on projects: Provide ample opportunities for students to practice coding using LLM-based tools. Assign hands-on projects that require students to apply their coding skills and utilize the LLM-based tools for code generation. This will help them gain practical experience and reinforce their learning.\n\n7. Supplement with traditional learning resources: While LLM-based tools can be valuable for code generation, it is important to supplement the learning process with traditional coding resources, textbooks, and tutorials. This will ensure a comprehensive understanding of coding principles and concepts.\n\nBy following this step-by-step approach, students can effectively use LLMs to learn coding. The combination of LLM-based tools and traditional learning resources can provide a well-rounded learning experience, enabling students to develop coding skills and proficiency.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notify the user that we are querying the index\n",
        "print(\"Querying the index...\")\n",
        "\n",
        "# Query the index for the provided question and store the response\n",
        "response = index.as_query_engine().query(\"\"\"\n",
        "Can you provide a detailed summary of the paper - \"SheetCopilot- Bringing Software Productivity to the Next Level through Large Language Models\".\n",
        "\"\"\")\n",
        "\n",
        "# Print the received response\n",
        "print(response)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-31T12:44:53.141511Z",
          "iopub.execute_input": "2023-07-31T12:44:53.142109Z",
          "iopub.status.idle": "2023-07-31T12:45:15.872305Z",
          "shell.execute_reply.started": "2023-07-31T12:44:53.142066Z",
          "shell.execute_reply": "2023-07-31T12:45:15.871068Z"
        },
        "trusted": true,
        "id": "XB5gpvUknTcH",
        "outputId": "77a29893-9869-48b2-9c2f-90d58d88ffa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Querying the index...\nThe paper \"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models\" introduces a system called SheetCopilot, which aims to enhance software productivity by using large language models (LLMs). The authors highlight that many computer end users spend a significant amount of time on repetitive and error-prone tasks, such as tabular data processing and project timeline scheduling. However, most users lack the skills to automate these tasks. With the advancements in LLMs, the authors propose using natural language user requests to direct software.\n\nThe SheetCopilot agent is designed to take natural language tasks and control spreadsheets to fulfill the requirements. The authors propose a set of atomic actions that serve as an abstraction of spreadsheet software functionalities. They also develop a state machine-based task planning framework to enable LLMs to interact robustly with spreadsheets. To evaluate the performance of LLMs in software control tasks, the authors curate a dataset containing 221 spreadsheet control tasks and establish an automated evaluation pipeline.\n\nThe results show that SheetCopilot successfully completes 44.3% of tasks in a single generation, outperforming a strong code generation baseline. The authors emphasize the potential of LLMs in directing complex software systems and highlight the need for a standardized framework for model-application interaction and a comprehensive benchmark for evaluating LLM performance.\n\nIn summary, the paper introduces SheetCopilot, a system that leverages large language models to enhance software productivity. The system demonstrates promising results in completing spreadsheet control tasks and highlights the potential of LLMs in directing complex software systems.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The End**"
      ],
      "metadata": {
        "id": "KhI0hgWWnf0R"
      }
    }
  ]
}